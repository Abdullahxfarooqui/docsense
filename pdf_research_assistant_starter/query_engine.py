"""
Query Engine Module with OpenRouter Integration

This module handles vector database querying, context retrieval, and LLM prompting
for the Research Assistant. It provides functionality to search through
vectorized document content and generate contextual answers using OpenRouter's API.

Author: AI Assistant
Created: August 2025
"""

import logging
import os
from typing import List, Tuple, Dict, Any, Optional
import traceback
import time
from datetime import datetime, timedelta

import streamlit as st
from langchain_openai import OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain.schema import Document
import chromadb
from dotenv import load_dotenv
from openai import OpenAI

from ingestion import get_collection, DocumentIngestionError

# Load environment variables
load_dotenv()

# Configure logging
logger = logging.getLogger(__name__)

# Constants
DEFAULT_MODEL = os.getenv("OPENAI_MODEL", "tngtech/deepseek-r1t2-chimera:free")
DEFAULT_TEMPERATURE = 0.1
MAX_RETRIES = 3
TOP_K_RESULTS = 3
CACHE_EXPIRY_MINUTES = 30


class QueryEngineError(Exception):
    """Custom exception for query engine errors."""
    pass


# Enhanced prompt template with explicit citation instructions
QA_PROMPT_TEMPLATE = """You are a highly knowledgeable research assistant. Your task is to answer questions based on the provided context from document files.

INSTRUCTIONS:
1. Use ONLY the information provided in the context below
2. ALWAYS cite your sources by mentioning the specific filename and chunk number
3. If the context doesn't contain enough information to answer the question, say so explicitly
4. Provide a comprehensive answer while being concise and accurate
5. Use the format: "According to [filename], chunk [number]: [information]"

CONTEXT:
{context}

QUESTION: {question}

ANSWER:"""

QA_PROMPT = PromptTemplate(
    input_variables=["context", "question"],
    template=QA_PROMPT_TEMPLATE
)


class QueryEngine:
    """
    A sophisticated query engine for semantic search and question answering
    over document collections using vector similarity and OpenRouter LLM reasoning.
    
    This class handles:
    - Vector similarity search using ChromaDB
    - Context preparation and formatting
    - OpenRouter-based answer generation with source citation
    - Caching of recent queries for performance
    - Error handling and retry logic
    """
    
    def __init__(self, model_name: str = DEFAULT_MODEL, temperature: float = DEFAULT_TEMPERATURE):
        """
        Initialize the QueryEngine with specified LLM configuration.
        
        Args:
            model_name: OpenRouter model to use for answer generation
            temperature: Sampling temperature for response generation (0.0 to 1.0)
            
        Raises:
            QueryEngineError: If initialization fails
        """
        try:
            self._validate_environment()
            
            self.model_name = model_name
            self.temperature = temperature
            
            # Get API configuration from environment
            self.api_key = os.getenv("OPENAI_API_KEY")
            self.base_url = os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1")
            self.site_url = os.getenv("SITE_URL", "http://localhost:8501")
            self.site_name = os.getenv("SITE_NAME", "Research Assistant")
            
            # Initialize OpenRouter client
            self.client = OpenAI(
                base_url=self.base_url,
                api_key=self.api_key
            )
            
            # Test the connection
            self._test_connection()
            
            # Initialize embeddings for query encoding
            # Note: For embeddings, we might need a different approach with OpenRouter
            self._init_embeddings()
            
            # Get ChromaDB collection
            self.collection = get_collection()
            
            # Initialize query cache
            if 'query_cache' not in st.session_state:
                st.session_state.query_cache = {}
            
            logger.info(f"QueryEngine initialized with model: {model_name}, temperature: {temperature}")
            
        except Exception as e:
            logger.error(f"Failed to initialize QueryEngine: {str(e)}")
            raise QueryEngineError(f"QueryEngine initialization failed: {str(e)}")
    
    def _validate_environment(self) -> None:
        """
        Validate that required environment variables are set.
        
        Raises:
            QueryEngineError: If required environment variables are missing
        """
        if not os.getenv("OPENAI_API_KEY"):
            raise QueryEngineError(
                "OPENAI_API_KEY environment variable is not set. "
                "Please set your OpenRouter API key before using the application."
            )
    
    def _test_connection(self) -> None:
        """
        Test the OpenRouter API connection.
        
        Raises:
            QueryEngineError: If connection test fails
        """
        try:
            response = self.client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": self.site_url,
                    "X-Title": self.site_name,
                },
                model=self.model_name,
                messages=[{"role": "user", "content": "Hello, testing connection."}],
                max_tokens=10
            )
            logger.info("OpenRouter API connection test successful")
        except Exception as e:
            logger.error(f"OpenRouter API connection test failed: {str(e)}")
            raise QueryEngineError(f"Failed to connect to OpenRouter API: {str(e)}")
    
    def _init_embeddings(self) -> None:
        """
        Initialize embeddings with fast fallback options.
        Prioritize speed over embedding quality for better user experience.
        """
        try:
            # Skip complex embedding initialization for now to improve performance
            # We'll use ChromaDB's direct text search which is much faster
            logger.info("Using fast text-based search for optimal performance")
            self.embeddings = None
            
        except Exception as e:
            logger.warning(f"Embedding initialization skipped for performance: {str(e)}")
            self.embeddings = None
    
    def _is_cache_valid(self, cache_entry: Dict[str, Any]) -> bool:
        """
        Check if a cache entry is still valid based on timestamp.
        
        Args:
            cache_entry: Dictionary containing cached data and timestamp
            
        Returns:
            bool: True if cache entry is still valid, False otherwise
        """
        if 'timestamp' not in cache_entry:
            return False
        
        cache_time = cache_entry['timestamp']
        expiry_time = cache_time + timedelta(minutes=CACHE_EXPIRY_MINUTES)
        
        return datetime.now() < expiry_time
    
    def _get_cached_result(self, question: str) -> Optional[Tuple[str, List[Tuple[str, str, Dict[str, Any]]]]]:
        """
        Retrieve cached result for a question if available and valid.
        
        Args:
            question: The user's question
            
        Returns:
            Cached result tuple or None if not available/valid
        """
        # Ensure query_cache is initialized
        if 'query_cache' not in st.session_state:
            st.session_state.query_cache = {}
            return None
            
        cache_key = question.lower().strip()
        
        if cache_key in st.session_state.query_cache:
            cache_entry = st.session_state.query_cache[cache_key]
            if self._is_cache_valid(cache_entry):
                logger.info(f"Returning cached result for question: {question[:50]}...")
                return cache_entry['result']
        
        return None
    
    def _cache_result(self, question: str, result: Tuple[str, List[Tuple[str, str, Dict[str, Any]]]]) -> None:
        """
        Cache the result of a query for future use.
        
        Args:
            question: The user's question
            result: The query result to cache
        """
        # Ensure query_cache is initialized
        if 'query_cache' not in st.session_state:
            st.session_state.query_cache = {}
            
        cache_key = question.lower().strip()
        
        st.session_state.query_cache[cache_key] = {
            'result': result,
            'timestamp': datetime.now()
        }
        
        # Limit cache size to prevent memory issues
        if len(st.session_state.query_cache) > 50:
            # Remove oldest entries
            sorted_cache = sorted(
                st.session_state.query_cache.items(),
                key=lambda x: x[1]['timestamp']
            )
            # Keep only the 30 most recent entries
            st.session_state.query_cache = dict(sorted_cache[-30:])
    
    def _retrieve_relevant_chunks(self, question: str, k: int = TOP_K_RESULTS) -> List[Dict[str, Any]]:
        """
        Retrieve the most relevant document chunks for a given question.
        
        Args:
            question: User's question
            k: Number of top results to retrieve
            
        Returns:
            List of dictionaries containing chunk data and metadata
            
        Raises:
            QueryEngineError: If retrieval fails
        """
        try:
            # Check if collection has any data
            collection_data = self.collection.get()
            if not collection_data['ids']:
                raise QueryEngineError(
                    "No documents found in the vector store. "
                    "Please upload and process some document files first."
                )
            
            logger.info(f"Searching through {len(collection_data['ids'])} document chunks")
            
            # Use ChromaDB's optimized text-based search directly for speed
            # This avoids the embedding generation overhead that was causing delays
            try:
                results = self.collection.query(
                    query_texts=[question],
                    n_results=min(k, len(collection_data['ids'])),
                    include=['documents', 'metadatas', 'distances']
                )
                logger.info(f"Fast text-based search completed successfully")
            except Exception as search_error:
                logger.error(f"ChromaDB search failed: {str(search_error)}")
                # Last resort: simple text matching
                logger.warning("Falling back to simple text matching")
                all_docs = collection_data['documents']
                all_metadata = collection_data['metadatas'] or [{}] * len(all_docs)
                all_ids = collection_data['ids']
                
                # Simple keyword matching for emergency fallback
                question_words = set(question.lower().split())
                scored_docs = []
                
                for i, doc in enumerate(all_docs):
                    doc_words = set(doc.lower().split())
                    score = len(question_words.intersection(doc_words)) / len(question_words.union(doc_words))
                    scored_docs.append((score, i))
                
                # Sort by score and take top k
                scored_docs.sort(reverse=True)
                top_indices = [idx for _, idx in scored_docs[:k]]
                
                results = {
                    'documents': [[all_docs[i] for i in top_indices]],
                    'metadatas': [[all_metadata[i] for i in top_indices]],
                    'ids': [[all_ids[i] for i in top_indices]],
                    'distances': [[1 - scored_docs[i][0] for i in range(len(top_indices))]]
                }
            
            if not results['documents'] or not results['documents'][0]:
                raise QueryEngineError(
                    "No relevant documents found for your question. "
                    "Try rephrasing your question or upload more relevant documents."
                )
            
            # Format results quickly
            retrieved_chunks = []
            for i in range(len(results['documents'][0])):
                chunk_data = {
                    'id': results['ids'][0][i] if 'ids' in results else f"chunk_{i}",
                    'content': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i] if results['metadatas'] else {},
                    'similarity_score': 1 - results['distances'][0][i] if results['distances'] else 0.0
                }
                retrieved_chunks.append(chunk_data)
            
            logger.info(f"Retrieved {len(retrieved_chunks)} relevant chunks for question")
            return retrieved_chunks
            
        except QueryEngineError:
            raise
        except Exception as e:
            logger.error(f"Error retrieving relevant chunks: {str(e)}")
            logger.error(traceback.format_exc())
            raise QueryEngineError(f"Failed to retrieve relevant documents: {str(e)}")
    
    def _format_context(self, chunks: List[Dict[str, Any]]) -> str:
        """
        Format retrieved chunks into a coherent context for the LLM.
        
        Args:
            chunks: List of retrieved chunk data
            
        Returns:
            Formatted context string with source citations
        """
        context_parts = []
        
        for i, chunk in enumerate(chunks):
            metadata = chunk.get('metadata', {})
            source = metadata.get('source', 'Unknown source')
            chunk_index = metadata.get('chunk_index', i)
            file_type = metadata.get('file_type', 'unknown')
            similarity = chunk.get('similarity_score', 0.0)
            
            context_part = f"""
Source: {source} ({file_type.upper()}) - Chunk {chunk_index + 1} (Similarity: {similarity:.3f})
Content: {chunk['content'].strip()}
"""
            context_parts.append(context_part)
        
        return "\n" + "="*50 + "\n".join(context_parts) + "\n" + "="*50
    
    def _generate_answer(self, question: str, context: str) -> str:
        """
        Generate an answer using OpenRouter's LLM based on the provided context.
        
        Args:
            question: User's question
            context: Formatted context from retrieved chunks
            
        Returns:
            Generated answer with source citations
            
        Raises:
            QueryEngineError: If answer generation fails
        """
        try:
            # Format the prompt
            prompt = QA_PROMPT.format(context=context, question=question)
            
            logger.info(f"Generating answer using model: {self.model_name}")
            
            # Generate answer with retry logic
            for attempt in range(MAX_RETRIES):
                try:
                    response = self.client.chat.completions.create(
                        extra_headers={
                            "HTTP-Referer": self.site_url,
                            "X-Title": self.site_name,
                        },
                        model=self.model_name,
                        messages=[
                            {"role": "user", "content": prompt}
                        ],
                        temperature=self.temperature,
                        max_tokens=1000
                    )
                    
                    answer = response.choices[0].message.content
                    
                    if not answer or not answer.strip():
                        raise QueryEngineError("Generated answer is empty")
                    
                    logger.info(f"Successfully generated answer for question")
                    return answer.strip()
                    
                except Exception as e:
                    if attempt < MAX_RETRIES - 1:
                        logger.warning(f"Answer generation attempt {attempt + 1} failed: {str(e)}. Retrying...")
                        continue
                    else:
                        raise
            
        except Exception as e:
            logger.error(f"Failed to generate answer: {str(e)}")
            logger.error(traceback.format_exc())
            raise QueryEngineError(f"Answer generation failed: {str(e)}")
    
    def answer_question(self, question: str) -> Tuple[str, List[Tuple[str, str, Dict[str, Any]]]]:
        """
        Process a user question and return an answer with source citations.
        Includes performance monitoring for search optimization.
        
        This method:
        1. Checks cache for recent identical queries
        2. Retrieves semantically relevant document chunks (optimized)
        3. Formats context for the LLM
        4. Generates an answer with proper source citations using OpenRouter
        5. Caches the result for future use
        
        Args:
            question: User's natural language question
            
        Returns:
            Tuple containing:
            - answer: Generated answer with source citations
            - sources: List of tuples (source_name, chunk_content, metadata)
            
        Raises:
            QueryEngineError: If question processing fails
        """
        start_time = time.time()
        
        try:
            if not question or not question.strip():
                raise QueryEngineError("Question cannot be empty")
            
            question = question.strip()
            logger.info(f"Processing question: {question[:100]}...")
            
            # Check cache first
            cache_start = time.time()
            cached_result = self._get_cached_result(question)
            if cached_result:
                cache_time = time.time() - cache_start
                logger.info(f"Cache hit - returned in {cache_time:.3f}s")
                return cached_result
            
            # Retrieve relevant chunks with performance tracking
            search_start = time.time()
            relevant_chunks = self._retrieve_relevant_chunks(question)
            search_time = time.time() - search_start
            logger.info(f"Search completed in {search_time:.3f}s")
            
            if not relevant_chunks:
                raise QueryEngineError("No relevant content found for your question")
            
            # Format context for LLM
            context_start = time.time()
            context = self._format_context(relevant_chunks)
            context_time = time.time() - context_start
            
            # Generate answer using OpenRouter
            generation_start = time.time()
            answer = self._generate_answer(question, context)
            generation_time = time.time() - generation_start
            logger.info(f"Answer generation completed in {generation_time:.3f}s")
            
            # Prepare source information for UI
            sources = []
            for chunk in relevant_chunks:
                metadata = chunk.get('metadata', {})
                source_name = metadata.get('source', 'Unknown source')
                chunk_content = chunk['content']
                
                # Include additional metadata for display
                enhanced_metadata = {
                    **metadata,
                    'similarity_score': chunk.get('similarity_score', 0.0),
                    'chunk_id': chunk.get('id', 'unknown')
                }
                
                sources.append((source_name, chunk_content, enhanced_metadata))
            
            result = (answer, sources)
            
            # Cache the result
            self._cache_result(question, result)
            
            total_time = time.time() - start_time
            
            # Store search time in session state for UI display
            st.session_state.last_search_time = total_time
            
            logger.info(f"Question processed successfully in {total_time:.3f}s total (search: {search_time:.3f}s, generation: {generation_time:.3f}s)")
            return result
            
        except QueryEngineError:
            raise
        except Exception as e:
            total_time = time.time() - start_time
            logger.error(f"Question processing failed after {total_time:.3f}s: {str(e)}")
            logger.error(traceback.format_exc())
            raise QueryEngineError(f"Failed to process question: {str(e)}")
    
    def get_query_stats(self) -> Dict[str, Any]:
        """
        Get statistics about cached queries and system status.
        
        Returns:
            Dictionary containing query engine statistics
        """
        try:
            # Ensure query_cache is initialized
            if 'query_cache' not in st.session_state:
                st.session_state.query_cache = {}
                
            collection_data = self.collection.get()
            cache_size = len(st.session_state.query_cache)
            
            return {
                "total_documents": len(collection_data.get('ids', [])),
                "cached_queries": cache_size,
                "model_name": self.model_name,
                "temperature": self.temperature,
                "top_k_results": TOP_K_RESULTS,
                "api_base_url": self.base_url
            }
            
        except Exception as e:
            logger.error(f"Failed to get query statistics: {str(e)}")
            return {"error": str(e)}
    
    def clear_cache(self) -> None:
        """Clear the query cache."""
        # Ensure query_cache is initialized before clearing
        if 'query_cache' not in st.session_state:
            st.session_state.query_cache = {}
        else:
            st.session_state.query_cache.clear()
        logger.info("Query cache cleared")


@st.cache_resource
def get_query_engine(model_name: str = DEFAULT_MODEL, temperature: float = DEFAULT_TEMPERATURE) -> QueryEngine:
    """
    Get or create a cached QueryEngine instance.
    
    Args:
        model_name: OpenRouter model to use
        temperature: Sampling temperature
        
    Returns:
        QueryEngine instance
    """
    return QueryEngine(model_name=model_name, temperature=temperature)